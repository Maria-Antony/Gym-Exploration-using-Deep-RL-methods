# Overview

This comprehensive project is a deep exploration into the realm of reinforcement learning, with a sharp focus on the application and meticulous analysis of two prominent algorithms: Deep Q-Network (DQN) and Double Deep Q-Network (DDQN). Centered within diverse Gym environments, this endeavor aims to dissect the performance, strengths, and limitations of these cutting-edge deep learning techniques in tackling complex decision-making tasks. Through rigorous experimentation and methodical analysis, this project seeks to unravel the intricacies of deep reinforcement learning, shedding light on its efficacy and adaptability in real-world scenarios.
It explores two gymnasium environments and a custom grid world environment.

# DQN(Deep Q-Network)
Deep Q-Network (DQN) stands as a seminal advancement in the domain of reinforcement learning, leveraging deep neural networks to approximate the optimal action-value function. By synthesizing states into action values, DQN empowers agents to navigate high-dimensional and continuous state spaces with precision. At its core, DQN entails the learning of a parameterized action-value function, typically manifested as a deep neural network, through the iterative minimization of temporal difference errors. This process involves continuous updates to the network parameters, facilitated by gradient descent techniques and guided by a fusion of observed experiences and target Q-values. DQN's prowess transcends across diverse domains, orchestrating complex robotic control tasks, underscoring its versatility and practical efficacy in real-world contexts.

# DDQN(Double Deep Q-Network)
Building upon the foundation laid by DQN, the Double Deep Q-Network (DDQN) algorithm emerges as a transformative evolution, equipped to address inherent limitations and augment overall performance. A pivotal enhancement introduced by DDQN lies in the segregation of action selection and value evaluation functions, facilitated by the utilization of dual neural networks. While one network orchestrates action selection (the policy network), the other undertakes value evaluation (the target network). This decoupling mechanism serves to alleviate overestimation bias, a common challenge encountered in Q-learning paradigms. Moreover, DDQN incorporates a sophisticated strategy termed target network updating, wherein the parameters of the target network undergo periodic updates to synchronize with those of the policy network. This stabilization mechanism fortifies the algorithm's resilience and convergence characteristics, culminating in heightened reliability and efficiency during the learning process. Through the amalgamation of these pioneering advancements, DDQN emerges as a formidable force in the landscape of deep reinforcement learning, poised to deliver enhanced performance and steadfastness in navigating intricate decision spaces.

# Environments
### CartPole
The CartPole environment is a seminal control problem within the RL community. The objective is to maintain the balance of a pole on a moving cart by exerting appropriate left or right forces. This environment serves as an excellent testbed for examining an agent's capability to develop stable control strategies through continuous real-time feedback and adjustment. The simplicity of the CartPole environment allows for rapid iteration and debugging, providing a clear visualization of the learning process and algorithmic performance.

### Acrobot
The Acrobot environment, a classic problem in reinforcement learning, presents a challenging scenario where an underactuated two-link pendulum system must swing its end-effector to a designated height. Represented within Gym, Acrobot embodies a quintessential example of a continuous state and discrete action space, posing significant challenges for traditional control strategies. With state transitions governed by physics-based dynamics and sparse rewards contingent on achieving specific angular goals, Acrobot demands nuanced decision-making and precise control to ascend beyond its inherent instability. Its technical complexity and inherent nonlinearity make it a compelling testbed for evaluating the efficacy of reinforcement learning algorithms, offering insights into the adaptability and robustness of agents in navigating high-dimensional and intricate environments.

### Custom Grid World Environment
In a 5x5 grid-based virtual environment inspired by "Courage the Cowardly Dog," the agent, embodying Courage, navigates to find an exit from a cave, symbolizing game completion. Four potential actions—moving up, down, left, or right—are available, facilitating comprehensive exploration. Encounters with obstacles like ghosts, zombies, and the grim reaper heighten complexity. To counterbalance challenges, strategic rewards such as torchlights, ghost-repelling pills, fire torches, and cloaks are placed within the grid, enhancing gameplay depth. The primary objective is to guide Courage to the exit, maximizing reward accumulation while efficiently navigating obstacles. This objective fosters adaptive learning through trial and error, with each action contributing to the agent's evolving strategy to optimize its path and balance risk with reward.

![Custom Grid World Environment](/images/Gridworld_env.png)
